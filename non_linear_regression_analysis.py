# -*- coding: utf-8 -*-
"""Non-linear Regression Analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bLXg_rM_HT7LQ5zG-TzRksg43YcemFPl
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline

"""Though Linear regression is very good to solve many problems, it cannot be used for all datasets. First recall how linear regression, could model a dataset. It models a linear relation between a dependent variable y and independent variable x. It had a simple equation, of degree 1, for example y = 2ğ‘¥ + 3."""

x = np.arange(-5.0, 5.0, 0.1)

##You can adjust the slope and intercept to verify the changes in the graph

y = 2*(x) + 3
y_noise = 2 * np.random.normal(size=x.size)
ydata = y + y_noise

#plt.figure(figsize=(8,6))

plt.plot(x, ydata,  'bo')
plt.plot(x,y, 'r') 
plt.ylabel('Dependent Variable')
plt.xlabel('Independent Variable')
plt.show()

"""Non-linear regressions are a relationship between independent variables ğ‘¥ and a dependent variable ğ‘¦ which result in a non-linear function modeled data. Essentially any relationship that is not linear can be termed as non-linear, and is usually represented by the polynomial of ğ‘˜ degrees (maximum power of ğ‘¥)

 ğ‘¦=ğ‘ğ‘¥3+ğ‘ğ‘¥2+ğ‘ğ‘¥+ğ‘‘ 

Non-linear functions can have elements like exponentials, logarithms, fractions, and others. For example:
ğ‘¦=log(ğ‘¥)

Or even, more complicated such as :
ğ‘¦=log(ğ‘ğ‘¥3+ğ‘ğ‘¥2+ğ‘ğ‘¥+ğ‘‘)

Let's take a look at a cubic function's graph.

"""

x = np.arange(-5.0, 5.0, 0.1)

##You can adjust the slope and intercept to verify the changes in the graph

y = 1*(x**3) + 1*(x**2) + 1*x + 3
y_noise = 20 * np.random.normal(size=x.size)
ydata = y + y_noise
plt.plot(x, ydata,  'bo')
plt.plot(x,y, 'r') 
plt.ylabel('Dependent Variable')
plt.xlabel('Independent Variable')
plt.show()

"""As you can see, this function has ğ‘¥3 and ğ‘¥2

as independent variables. Also, the graphic of this function is not a straight line over the 2D plane. So this is a non-linear function.

Some other types of non-linear functions are:
Quadratic

ğ‘Œ=ğ‘‹2
"""

x = np.arange(-5.0, 5.0, 0.1)

##You can adjust the slope and intercept to verify the changes in the graph

y = np.power(x,2)
y_noise = 2 * np.random.normal(size=x.size)
ydata = y + y_noise
plt.plot(x, ydata,  'bo')
plt.plot(x,y, 'r') 
plt.ylabel('Dependent Variable')
plt.xlabel('Independent Variable')
plt.show()

"""Exponential

An exponential function with base c is defined by
ğ‘Œ=ğ‘+ğ‘ğ‘ğ‘‹

where b â‰ 0, c > 0 , c â‰ 1, and x is any real number. The base, c, is constant and the exponent, x, is a variable.

"""

X = np.arange(-5.0, 5.0, 0.1)

##You can adjust the slope and intercept to verify the changes in the graph

Y= np.exp(X)

plt.plot(X,Y) 
plt.ylabel('Dependent Variable')
plt.xlabel('Independent Variable')
plt.show()

"""Logarithmic

The response ğ‘¦
is a results of applying logarithmic map from input ğ‘¥'s to output variable ğ‘¦. It is one of the simplest form of log(): i.e.
ğ‘¦=log(ğ‘¥)

Please consider that instead of ğ‘¥
, we can use ğ‘‹, which can be polynomial representation of the ğ‘¥'s. In general form it would be written as
ğ‘¦=log(ğ‘‹)
"""

X = np.arange(-5.0, 5.0, 0.1)

Y = np.log(X)

plt.plot(X,Y)
plt.ylabel("Dependent Variable")
plt.xlabel("Independent Variable")
plt.show()

"""Sigmoidal/Logistic

ğ‘Œ=ğ‘+ğ‘1+ğ‘(ğ‘‹âˆ’ğ‘‘)
"""

X = np.arange(-5.0, 5.0, 0.1)


Y = 1-4/(1+np.power(3, X-2))

plt.plot(X,Y) 
plt.ylabel('Dependent Variable')
plt.xlabel('Independent Variable')
plt.show()

"""Non-Linear Regression example

For an example, we're going to try and fit a non-linear model to the datapoints corresponding to China's GDP from 1960 to 2014. We download a dataset with two columns, the first, a year between 1960 and 2014, the second, China's corresponding annual gross domestic income in US dollars for that year.
"""

url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%202/data/china_gdp.csv"

df = pd.read_csv(url)
df.describe()
df.head(10)

# Plotting the Dataset

plt.figure(figsize=(8,5))
x_data, y_data = (df["Year"].values, df["Value"].values)
plt.plot(x_data, y_data, 'ro')
plt.ylabel('GDP')
plt.xlabel('Year')
plt.show()

"""Choosing a model

From an initial look at the plot, we determine that the logistic function could be a good approximation, since it has the property of starting with a slow growth, increasing growth in the middle, and then decreasing again at the end; as illustrated below:

"""

X = np.arange(-5.0, 5.0, 0.1)
Y = 1.0 / (1.0 + np.exp(-X))

plt.plot(X,Y) 
plt.ylabel('Dependent Variable')
plt.xlabel('Independent Variable')
plt.show()

"""The formula for the logistic function is the following:

ğ‘ŒÌ‚ =11+ğ‘’ğ›½1(ğ‘‹âˆ’ğ›½2)

ğ›½1

: Controls the curve's steepness,

ğ›½2
: Slides the curve on the x-axis.
"""

# Building The Model, initialize its parameter

def sigmoid(x, Beta_1, Beta_2):
     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))
     return y

# Lets look at a sample sigmoid line that might fit with the data:

beta_1 = 0.10
beta_2 = 1990.0

#logistic function
Y_pred = sigmoid(x_data, beta_1 , beta_2)

#plot initial prediction against datapoints
plt.plot(x_data, Y_pred*15000000000000.)
plt.plot(x_data, y_data, 'ro')

# Our task here is to find the best parameters for our model. Lets first normalize our x and y:

# Lets normalize our data

xdata =x_data/max(x_data)
ydata =y_data/max(y_data)

"""How we find the best parameters for our fit line?

we can use curve_fit which uses non-linear least squares to fit our sigmoid function, to data. Optimal values for the parameters so that the sum of the squared residuals of sigmoid(xdata, *popt) - ydata is minimized.

popt are our optimized parameters.

"""

from scipy.optimize import curve_fit
popt, pcov = curve_fit(sigmoid, xdata, ydata)
#print the final parameters
print(" beta_1 = %f, beta_2 = %f" % (popt[0], popt[1]))

"""Now we plot our resulting regression model.

"""

x = np.linspace(1960, 2015, 55)
x = x/max(x)
plt.figure(figsize=(8,5))
y = sigmoid(x, *popt)
plt.plot(xdata, ydata, 'ro', label='data')
plt.plot(x,y, linewidth=3.0, label='fit')
plt.legend(loc='best')
plt.ylabel('GDP')
plt.xlabel('Year')
plt.show()